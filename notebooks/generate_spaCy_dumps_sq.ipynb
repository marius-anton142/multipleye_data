{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q17gmm7G6VZX"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAf08EAe6WPg",
        "outputId": "f7d91b83-7fcc-45c7-bdfb-a9b7b3de774f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "params: ['sq'] data/dump\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "LANGS = [x.strip() for x in os.getenv(\"LANGS\", \"sq\").split(\",\") if x.strip()]\n",
        "OUT_DIR = os.getenv(\"OUT_DIR\", \"data/dump\").strip()\n",
        "\n",
        "print(\"params:\", LANGS, OUT_DIR)\n",
        "os.makedirs(OUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHhXRZOf7aIx"
      },
      "source": [
        "## Get spacy models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGNUWOHg7mKz",
        "outputId": "f56a0afc-323f-42d9-bf90-e8d0d316c4ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xx-sent-ud-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/xx_sent_ud_sm-3.8.0/xx_sent_ud_sm-3.8.0-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xx-sent-ud-sm\n",
            "Successfully installed xx-sent-ud-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('xx_sent_ud_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "use blank\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download xx_sent_ud_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3J7nrCc7ebN"
      },
      "source": [
        "## Get the MultiplEYE json data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4L0uFaHSk-y"
      },
      "outputs": [],
      "source": [
        "! rm -rf languages*\n",
        "! wget https://github.com/senisioi/repository/releases/download/eyelanguages0/languages_json_all.zip\n",
        "! unzip languages_json_all.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0xc3i_xvQU1Z"
      },
      "outputs": [],
      "source": [
        "SPACY_LANGUAGES = []\n",
        "\n",
        "CODE2LANG = {\n",
        "    \"sq\": \"Albanian\",\n",
        "}\n",
        "\n",
        "LANGUAGES = list(CODE2LANG.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDPp5lF77uB2"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "R27JzBYj9iIw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import spacy\n",
        "\n",
        "def load_all_json(lang_folder):\n",
        "    all_data = {}\n",
        "    for file in os.listdir(lang_folder):\n",
        "        if file.endswith('.json'):\n",
        "            lang_code = file.replace('.json', '').replace('multipleye_stimuli_experiment_', '')\n",
        "            if lang_code == 'zd':\n",
        "                lang_code = 'gsw'\n",
        "            if (lang_code not in LANGUAGES) or (lang_code not in LANGS):\n",
        "                continue\n",
        "            with open(os.path.join(lang_folder, file), 'r', encoding='utf-8') as f:\n",
        "                all_data[lang_code] = json.load(f)\n",
        "    return all_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqdZjNWw9i6C",
        "outputId": "da79fba3-9ff5-4dfd-e4c2-200db5747ef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sq {'stimulus_id': 1, 'stimulus_name': 'PopSci_MultiplEYE', 'stimulus_type': 'experiment', 'pages': ['Projekti MultiplEYE\\n\\nEmri “MultiplEYE” është një lojë fjalësh që kombinon “multilingualism”\\n(shumëgjuhësinë) apo “multiple languages”  (gjuhët e shumta) me “sy”, duke iu\\nreferuar “eye-tracking” (gjurmimit të syve). MultiplEYE është një COST Action i\\nfinancuar nga Bashkimi Evropian. COST Actions janë rrjete kërkimore të mbështetura\\nnga Bashkëpunimi Evropian në Shkencë dhe Teknologji ose shkurt COST. Si organizatë\\nfinancuese, COST mbështet rrjetin tonë kërkimor në rritje në Evropë dhe më gjerë,\\nduke ofruar ndihmë financiare për aktivitete të ndryshme.', 'Këto aktivitete përfshijnë takimet e grupeve të punës, shkollat e trajnimit për të\\ntrajnuar për aftësi të reja studiuesit e rinj si edhe vizita kërkimore shkencore.\\nTitulli i projektit COST MultiplEYE nënkupton: Mundësimi i mbledhjes së të dhënave\\nshumëgjuhëshe përmes gjurmimit të syve për hulumtim rreth përpunimit të gjuhës\\nnjerëzore dhe asaj kompjuterike. Kjo do të thotë se MultiplEYE COST Action synon\\ntë nxisë një rrjet ndërdisiplinor të grupeve kërkimore që punojnë në mbledhjen e\\ntë dhënave të gjurmimit të syrit nga leximi në shumë gjuhë.', \"Qëllimi është të mbështetet zhvillimi i një korpusi të madh shumëgjuhësh për\\ngjurmimin e syve dhe t'u mundësohet studiuesve të mbledhin të dhëna duke ndarë\\nnjohuritë e tyre sipas fushave të ndryshme, duke përfshirë gjuhësinë,\\npsikologjinë, patologjinë e të folurit dhe gjuhës dhe shkencën kompjuterike. Ky\\nkoleksion i të dhënave mund të përdoret më pas për të studiuar përpunimin e gjuhës\\nnjerëzore nga një këndvështrim psikolinguistik, si dhe për të përmirësuar dhe\\nvlerësuar përpunimin e gjuhës kompjuterike në këndvështrimin e të mësuarit përmes\\nkompjuterit.\", 'Çfarë është \"gjurmimi i syve\"? Gjurmimi i syve është procesi i matjes së pikës së\\nshikimit - ku po shikoni - dhe lëvizjeve të syve midis pikave të fiksuara të\\nshikimit. Pajisja që përdoret për të matur pozicionin e syve dhe lëvizjet e syve\\nquhet gjurmues i syve. Ajo përbëhet nga një kamerë me rreze infra të kuqe, e\\ncila përdor një frekuencë drite që nuk shqetëson apo dëmton syrin e njeriut.', 'Me ndihmën e algoritmeve të njohjes së imazhit, gjurmuesi i syve mund të vlerësojë\\npikat e shikimit me shumë saktësi duke ditur pozicionin e kokës dhe syve,\\ndistancën nga ekrani që shikon një pjesëmarrës dhe pozicionin e gjurmuesit të\\nsyrit. Gjurmimi i syve është një teknologji e dobishme për shumë aplikacione. Për\\nshembull, mund të ndihmojë në zbulimin e lodhjes gjatë drejtimit të automjetit ose\\nmund të mbështesë aplikacione për qëllime ekzaminimi dhe trajnimi në fushën\\nmjekësore. Gjurmimi i syve përdoret gjithashtu në lojëra, marketing dhe\\nndërveprimin njeri-kompjuter.', 'Pse gjurmimi i syve gjatë leximit është veçanërisht interesante për projektin tonë?\\nNdërsa lexoni këto fjalë, gjurmuesi i syve ndjek lëvizjet e syrit tuaj mbi tekst.\\nKjo jep informacion rreth faktit se sa kohë shpenzoni duke parë një tekst, ose më\\nkonkretisht, sa kohë keni shpenzuar për secilën fjalë, cilat fjalë i keni\\nanashkaluar, në cilat fjalë jeni ndalur dhe nëse ju është dashur të ktheheni dhe\\ntë rilexoni pjesë të tekstit që ta kuptoni më mirë atë.', 'Ndërsa truri juaj është duke përpunuar njëkohësisht përmbajtjen e tekstit,\\nlëvizjet e syve tuaj pasqyrojnë shumë nga përpunimi gjuhësor dhe njohës që ndodh\\nnë kohë reale. Kështu, të dhënat e regjistruara janë një burim i çmuar me \\ninformacion rreth mënyrës se si ne bashkojmë kuptimin dhe strukturat gramatikore\\ntë një teksti. Ai tregon se me cilat pjesë të tekstit kemi vështirësi dhe cilat\\npjesë janë lehtësisht të lexueshme. U takon studiuesve të shpjegojnë më \\nvonë se cilët faktorë gjuhësorë shkaktuan llojin e lëvizjeve të syve.', 'Motivimi në lidhje me MultiplEYE është se të dhënat e gjurmimit të syrit janë ende\\ntë pakta, veçanërisht për gjuhët me më pak folës. Mbledhja e tillë e gjerë e të\\ndhënave është një sfidë për sa i përket zhvillimit të eksperimentit dhe dizajnit\\neksperimental, kompleksitetit dhe llojet e teksteve që do të lexohen nga\\npjesëmarrësit. Vendime të tjera që duken pak të rëndësishme, por në fakt janë\\nshumë të rëndësishme përfshijnë llojin e shkronjave dhe madhësinë në të cilën\\nparaqitet teksti, renditjen e teksteve, procedurën e eksperimentit dhe mënyrën se\\nsi do të përpunohen të dhënat.', 'Por, pasi të përfundojë, ky grup të dhënash do të na lejojë të hulumtojmë shumë\\ntema që lidhen me psikolinguistikën dhe gjuhësinë kompjuterike. Për shembull, ne\\nmund të krahasojmë sjelljen e leximit në gjuhë të ndryshme. A ndikon shkrimi, \\npër shembull, alfabeti latin kundrejt shkrimeve cirilike ose arabe, në\\nkohën e leximit? Një shembull në lidhje me përpunimin kompjuterik të tekstit mund\\ntë përfshijë përdorimin e të dhënave të gjurmimit të syrit për të avancuar\\naplikacionet e inteligjencës artificiale që imitojnë procesin e leximit njerëzor.\\nKjo mund të përdoret për të ndërtuar sisteme më të mira përkthimi kompjuterik ose\\npër të përmirësuar nxjerrjen automatike të fjalëve kyçe nga teksti.', 'Marrja e të dhënave të gjurmimit të syve nga shumë pjesëmarrës, duke përfshirë\\nedhe veten tuaj, duke lexuar tekste në shumë gjuhë të ndryshme do të jetë një bazë\\ne shkëlqyer për kërkimin tonë. Do të jetë faktori kryesor për ta kthyer rrjetin\\ntonë kërkimor në një përpjekje të suksesshme. Shpresojmë të hapim rrugën për\\navancimin e kërkimit në nënfusha të ndryshme të gjuhësisë duke mbështetur dhe\\nafruar një grup të madh studiuesish në këtë rrjet kërkimor.', 'Rezultatet kryesore të MultiplEYE COST Action do të jenë një grup i madh të\\ndhënash që përmban të dhëna të gjurmimit të syrit në shumë gjuhë dhe një\\nplatformë për bashkëpunime të reja të ndërtuara mbi këtë lloj të dhënash. \\nNëse po e lexoni këtë tekst, ju tashmë po mbështesni kauzën tonë duke na lejuar\\ntë mbledhim dhe analizojmë lëvizjet e syve tuaj gjatë leximit dhe të kuptuarit\\ntë gjuhës.\\nFaleminderit!']}\n"
          ]
        }
      ],
      "source": [
        "all_data = load_all_json('languages_json')\n",
        "for k,v in all_data.items():\n",
        "  print(k, v[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5Z0HnuEDdxt"
      },
      "source": [
        "## Prepare spaCy code to generate template csv files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Hnvn1bCFWLF_"
      },
      "outputs": [],
      "source": [
        "LANG_FOLDER = \"languages_json\"\n",
        "NLP_MODEL = None\n",
        "CURRENT_LANG = ''\n",
        "IN_DIR = 'languages_json/'\n",
        "\n",
        "from spacy.util import get_lang_class\n",
        "\n",
        "\n",
        "def exists_spacy_blank(lang_code):\n",
        "    try:\n",
        "        get_lang_class(lang_code)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def load_spacy_model(lang_code, small=True):\n",
        "    model = None\n",
        "    if lang_code in SPACY_LANGUAGES:\n",
        "        genre = 'news'\n",
        "        if lang_code in {'zh', 'en'}:\n",
        "            genre = 'web'\n",
        "        if lang_code == 'rm':\n",
        "            return ''\n",
        "        model_name = f'{lang_code}_core_{genre}_{\"sm\" if small else \"lg\"}'\n",
        "        print(f\"Loading model {model_name} for {lang_code}\")\n",
        "        model = spacy.load(model_name)\n",
        "        model.add_pipe(\"sentencizer\")\n",
        "    elif lang_code == \"rm\":\n",
        "        model = spacy.load(\"it_core_news_lg\")\n",
        "        # keep 'morphologizer' ?\n",
        "        model.disable_pipes('tok2vec', 'tagger', 'parser', 'lemmatizer', 'attribute_ruler', 'ner')\n",
        "    elif lang_code == 'gsw':\n",
        "        model = spacy.load('de_core_news_lg')\n",
        "    elif exists_spacy_blank(lang_code):\n",
        "        print(f\"Loading model blank model for {lang_code}\")\n",
        "        model = spacy.blank(lang_code)\n",
        "        model.add_pipe(\"sentencizer\")\n",
        "    else:\n",
        "        model_name = f'xx_sent_ud_sm'\n",
        "        print(f\"Loading model {model_name} for {lang_code}\")\n",
        "        model = spacy.load(model_name)\n",
        "        model.add_pipe(\"sentencizer\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_nlp(lang_code, small=False):\n",
        "    \"\"\"To avoid loading all models at the same time\n",
        "    \"\"\"\n",
        "    global NLP_MODEL, CURRENT_LANG\n",
        "    if lang_code != CURRENT_LANG:\n",
        "        try:\n",
        "            print(f\"Deleting model for {CURRENT_LANG}\")\n",
        "            del NLP_MODEL\n",
        "        except:\n",
        "            print(\"No model to delete\")\n",
        "        print(f\"Loading model for {lang_code}\")\n",
        "        NLP_MODEL = load_spacy_model(lang_code, small=small)\n",
        "        CURRENT_LANG = lang_code\n",
        "    return NLP_MODEL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "13jbiTKgbx4O"
      },
      "outputs": [],
      "source": [
        "from spacy.symbols import ORTH\n",
        "import re\n",
        "\n",
        "def feats_str(token):\n",
        "    if not token.morph:\n",
        "        return \"_\"\n",
        "    md = token.morph.to_dict()\n",
        "    if not md:\n",
        "        return \"_\"\n",
        "    bits = []\n",
        "    for k in sorted(md):\n",
        "        v = md[k]\n",
        "        if isinstance(v, (list, tuple)):\n",
        "            bits.append(f\"{k}={','.join(v)}\")\n",
        "        else:\n",
        "            bits.append(f\"{k}={v}\")\n",
        "    return \"|\".join(bits) if bits else \"_\"\n",
        "\n",
        "\n",
        "def get_head(token, sent):\n",
        "    if token.head == token or token.dep_ == \"ROOT\":\n",
        "        head = 0\n",
        "        deprel = \"root\"\n",
        "    else:\n",
        "        head = (token.head.i - sent.start) + 1  # 1-based in sentence\n",
        "        deprel = token.dep_.lower() if token.dep_ else \"_\"\n",
        "    return head, deprel\n",
        "\n",
        "\n",
        "def get_misc(token, include_ner=True):\n",
        "    misc_parts = []\n",
        "    if not token.whitespace_:\n",
        "        misc_parts.append(\"SpaceAfter=No\")\n",
        "    if include_ner and token.ent_iob_ != \"O\":\n",
        "        misc_parts.append(f\"NER={token.ent_iob_}-{token.ent_type_}\")\n",
        "    misc = \"|\".join(misc_parts) if misc_parts else \"_\"\n",
        "    return misc\n",
        "\n",
        "\n",
        "def iter_pages(stimuli, nlp):\n",
        "    for stim in stimuli:\n",
        "        sid, sname = stim[\"stimulus_id\"], stim[\"stimulus_name\"]\n",
        "        for pnum, page_text in enumerate(stim[\"pages\"], start=1):\n",
        "            yield sid, sname, pnum, nlp(page_text)\n",
        "\n",
        "def stimuli2csv(stimuli, lang_code, level=\"page\", small=False):\n",
        "    rows = []\n",
        "    nlp = get_nlp(lang_code, small=small)\n",
        "\n",
        "    pat = re.compile(r\"^([tTsS])['’](\\w+)$\", re.UNICODE)\n",
        "\n",
        "    @spacy.Language.component(\"split_t_s_apostrophe\")\n",
        "    def split_t_s_apostrophe(doc):\n",
        "        with doc.retokenize() as retok:\n",
        "            for tok in doc:\n",
        "                m = pat.match(tok.text)\n",
        "                if m:\n",
        "                    first = tok.text[:2]\n",
        "                    rest  = tok.text[2:]\n",
        "                    retok.split(tok, [first, rest], [tok.head, tok.head])\n",
        "        return doc\n",
        "\n",
        "    nlp.add_pipe(\"split_t_s_apostrophe\", first=True)\n",
        "\n",
        "    special_cases = {\"eye-tracking\" : [{ORTH: \"eye-tracking\"}],\n",
        "                     \"Ottenskjold-it\" : [{ORTH: \"Ottenskjold-it\"}],\n",
        "                     \"Ottenskjold-i\" : [{ORTH: \"Ottenskjold-i\"}],\n",
        "                     \"BE-së\" : [{ORTH: \"BE-së\"}]}\n",
        "\n",
        "    for token, special_case in special_cases.items():\n",
        "        nlp.tokenizer.add_special_case(token, special_case)\n",
        "\n",
        "    for sid, sname, page, doc in iter_pages(stimuli, nlp):\n",
        "        ptext = doc.text\n",
        "        document = nlp(ptext)\n",
        "        for sent_idx, sentence in enumerate(document.sents):\n",
        "            eos = {\n",
        "              \"language\": CODE2LANG[lang_code],\n",
        "              \"language_code\": lang_code,\n",
        "              \"stimulus_name\": sname,\n",
        "              \"page\": page,\n",
        "              #\"sent_idx\": sent_idx+1,\n",
        "              \"token\": \"<eos>\",\n",
        "              \"is_alpha\": False,\n",
        "              \"is_stop\": False,\n",
        "              \"is_punct\": False,\n",
        "              \"lemma\": \"\",\n",
        "              \"upos\": \"\",\n",
        "              \"xpos\": \"\",\n",
        "              \"feats\": \"\",\n",
        "              \"head\": \"\",\n",
        "              \"deprel\": \"\",\n",
        "              \"deps\": \"\",\n",
        "              \"misc\": \"\"\n",
        "              }\n",
        "            for token in sentence:\n",
        "                head, deprel = get_head(token, sentence)\n",
        "                rows.append(\n",
        "                    {\n",
        "                        #\"stimulus_id\": sid,\n",
        "                        \"language\": CODE2LANG[lang_code],\n",
        "                        \"language_code\": lang_code,\n",
        "                        \"stimulus_name\": sname,\n",
        "                        \"page\": page,\n",
        "                        #\"sent_idx\": sent_idx+1,\n",
        "                        \"token\": token.text,\n",
        "                        \"is_alpha\": token.is_alpha,\n",
        "                        \"is_stop\": token.is_stop,\n",
        "                        \"is_punct\": token.is_punct,\n",
        "                        \"lemma\": token.lemma_,\n",
        "                        \"upos\": token.pos_,\n",
        "                        \"xpos\": token.tag_,\n",
        "                        \"feats\": feats_str(token),\n",
        "                        \"head\": head,\n",
        "                        \"deprel\": deprel,\n",
        "                        \"deps\": \"_\",\n",
        "                        \"misc\": get_misc(token, include_ner=True)\n",
        "                    }\n",
        "                )\n",
        "            # rows.append(eos)\n",
        "\n",
        "    df = pd.DataFrame(rows).sort_values(by=[\"stimulus_name\", \"page\"])\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE4-VqxpOrt_"
      },
      "source": [
        "## Generate csv templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMb69Drkbx7D",
        "outputId": "54e1d30b-7af4-4ad5-9d10-010019660ff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleting model for \n",
            "Loading model for sq\n",
            "Loading model blank model for sq\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "preproc = defaultdict(dict)\n",
        "for lang_code, data in tqdm(all_data.items()):\n",
        "    if lang_code not in LANGS:\n",
        "        continue\n",
        "    preproc[lang_code] = stimuli2csv(data, lang_code, small=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkFLQik9U-sm"
      },
      "source": [
        "## Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBjlX2JFURsg",
        "outputId": "18b66759-2661-40e2-9c96-6a484713b444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 14.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/dump/sq/Arg_PISACowsMilk.csv\n",
            "data/dump/sq/Arg_PISARapaNui.csv\n",
            "data/dump/sq/Enc_WikiMoon.csv\n",
            "data/dump/sq/Ins_HumanRights.csv\n",
            "data/dump/sq/Ins_LearningMobility.csv\n",
            "data/dump/sq/Lit_Alchemist.csv\n",
            "data/dump/sq/Lit_BrokenApril.csv\n",
            "data/dump/sq/Lit_MagicMountain.csv\n",
            "data/dump/sq/Lit_NorthWind.csv\n",
            "data/dump/sq/Lit_Solaris.csv\n",
            "data/dump/sq/PopSci_Caveman.csv\n",
            "data/dump/sq/PopSci_MultiplEYE.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "for lang_code, df in tqdm(preproc.items()):\n",
        "    lang_out = 'gsw' if lang_code == 'zd' else lang_code\n",
        "    out_dir = os.path.join(OUT_DIR, lang_out)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    for stim_name, group in df.groupby('stimulus_name'):\n",
        "        out_fis = os.path.join(out_dir, f\"{stim_name}.csv\")\n",
        "        g = group.copy()\n",
        "        g['language_code'] = lang_out\n",
        "        g.to_csv(out_fis, index=False)\n",
        "        print(out_fis)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "shutil.make_archive(\"/content/sq_dump\", \"zip\", \"/content/data/dump/sq\")\n",
        "files.download(\"/content/sq_dump.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "dOG5uhR5Si5E",
        "outputId": "3ca56543-448c-4365-fda7-0e86b7a5018e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8f6026ba-ff40-48e3-86b7-f3388bc42764\", \"sq_dump.zip\", 38868)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.makedirs(\"/content\", exist_ok=True)\n",
        "\n",
        "# for lang_code, df in preproc.items():\n",
        "#     if lang_code not in LANGS:\n",
        "#         continue\n",
        "#     out_path = f\"/content/tokens_{lang_code}.csv\"\n",
        "#     df[\"token\"].to_csv(out_path, index=False, header=False)\n",
        "#     print(out_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Llh1xmOJ__zK",
        "outputId": "9bb1cd51-f9ea-4557-f6a7-a7d7dbb3020c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tokens_sq.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}