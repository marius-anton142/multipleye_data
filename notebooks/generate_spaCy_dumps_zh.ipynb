{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q17gmm7G6VZX"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAf08EAe6WPg",
        "outputId": "e06cd0d5-7546-43e3-e872-be8a9f4669b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "params: ['en', 'zh'] data/dump\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "LANGS = [x.strip() for x in os.getenv(\"LANGS\", \"en,zh\").split(\",\") if x.strip()]\n",
        "OUT_DIR = os.getenv(\"OUT_DIR\", \"data/dump\").strip()\n",
        "\n",
        "print(\"params:\", LANGS, OUT_DIR)\n",
        "os.makedirs(OUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHhXRZOf7aIx"
      },
      "source": [
        "## Get spacy models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGNUWOHg7mKz"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download xx_sent_ud_sm\n",
        "\n",
        "language_models = {\n",
        "    \"en\": \"en_core_web_lg\",\n",
        "    \"zh\": \"zh_core_web_lg\",\n",
        "}\n",
        "\n",
        "for lm in LANGS:\n",
        "    if lm in language_models:\n",
        "        print(f\"downloading {lm}: {language_models[lm]}\")\n",
        "        !python -m spacy download {language_models[lm]}\n",
        "    else:\n",
        "        print(f\"use blank\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3J7nrCc7ebN"
      },
      "source": [
        "## Get the MultiplEYE json data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4L0uFaHSk-y"
      },
      "outputs": [],
      "source": [
        "! rm -rf languages*\n",
        "! wget https://github.com/senisioi/repository/releases/download/eyelanguages0/languages_json_all.zip\n",
        "! unzip languages_json_all.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xc3i_xvQU1Z"
      },
      "outputs": [],
      "source": [
        "SPACY_LANGUAGES = [\"ca\", \"de\", \"el\", \"en\", \"es\", \"fr\", \"hr\", \"it\", \"lt\", \"mk\", \"nl\", \"pl\", \"pt\", \"ro\", \"ru\", \"sl\", \"sv\", \"uk\", \"zh\"]\n",
        "\n",
        "CODE2LANG = {\n",
        "    \"ar\": \"Arabic\",\n",
        "    \"ca\": \"Catalan\",\n",
        "    \"cs\": \"Czech\",\n",
        "    \"de\": \"German\",\n",
        "    \"gsw\": \"Swiss German\",\n",
        "    \"el\": \"Greek\",\n",
        "    \"en\": \"English\",\n",
        "    #\"es\": \"Spanish\",\n",
        "    \"et\": \"Estonian\",\n",
        "    \"eu\": \"Basque\",\n",
        "    #\"fr\": \"French\",\n",
        "    #\"he\": \"Hebrew\",\n",
        "    \"hi\": \"Hindi\",\n",
        "    \"hr\": \"Croatian\",\n",
        "    \"it\": \"Italian\",\n",
        "    \"kl\": \"Kalaallisut\",\n",
        "    \"lt\": \"Lithuanian\",\n",
        "    \"lv\": \"Latvian\",\n",
        "    \"mk\": \"Macedonian\",\n",
        "    \"nl\": \"Dutch\",\n",
        "    \"pl\": \"Polish\",\n",
        "    \"pt\": \"Portuguese\",\n",
        "    \"rm\": \"Romansh\",\n",
        "    \"ro\": \"Romanian\",\n",
        "    \"ru\": \"Russian\",\n",
        "    \"sl\": \"Slovenian\",\n",
        "    \"sq\": \"Albanian\",\n",
        "    \"sv\": \"Swedish\",\n",
        "    \"tr\": \"Turkish\",\n",
        "    \"uk\": \"Ukrainian\",\n",
        "    #\"yue\": \"Cantonese\",\n",
        "    \"zh\": \"Chinese\"\n",
        "}\n",
        "\n",
        "LANGUAGES = list(CODE2LANG.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDPp5lF77uB2"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R27JzBYj9iIw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import spacy\n",
        "\n",
        "def load_all_json(lang_folder):\n",
        "    all_data = {}\n",
        "    for file in os.listdir(lang_folder):\n",
        "        if file.endswith('.json'):\n",
        "            lang_code = file.replace('.json', '').replace('multipleye_stimuli_experiment_', '')\n",
        "            if lang_code == 'zd':\n",
        "                lang_code = 'gsw'\n",
        "            if (lang_code not in LANGUAGES) or (lang_code not in LANGS):\n",
        "                continue\n",
        "            with open(os.path.join(lang_folder, file), 'r', encoding='utf-8') as f:\n",
        "                all_data[lang_code] = json.load(f)\n",
        "    return all_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqdZjNWw9i6C",
        "outputId": "9d5890c9-daf0-4998-e33a-64675aae602f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "zh {'stimulus_id': 1, 'stimulus_name': 'PopSci_MultiplEYE', 'stimulus_type': 'experiment', 'pages': ['MultiplEYE项目\\n\\n“MultiplEYE”这个名字玩了一些文字游戏，把“multilingualism”（多语言）或者“multiple languages”（多种语言）和“eye-tracking”（眼动追踪）中的“eye”（眼睛）巧妙地结合在一起。MultiplEYE是由欧盟资助的COST行动项目。COST行动项目是欧洲科学与技术合作组织（简称COST）支持的研究网络。作为资助机构，COST在欧洲和其他地方提供财务援助，用于扩大研究人员网络，及支持我们的研究人员进行各种合作活动。', '这些活动包括工作组会议、培训年轻研究人员学习新技能的培训学校以及科学研究访问。MultiplEYE COST行动的项目名称是：为人类和机器语言处理研究收集多语言眼动追踪数据收集。这意味着MultiplEYE COST行动旨在建立一个跨学科研究小组网络，收集多语言阅读的眼动追踪数据。', '行动的目标是支持大型多语言眼动追踪语料库的开发，并使研究人员能够在语言学、心理学、言语病理学以及计算机科学等各个领域共享知识，由此来收集数据。这些数据可以在之后用于从心理语言学的角度研究人类语言处理，也可以有助于从机器学习角度改进和评估计算机的语言处理能力。', '那么，什么是“眼动追踪”呢？\\n简单说，眼动追踪就是测量人们看向哪里，眼睛是如何在不同点之间移动的过程。我们用一种叫做“眼动追踪仪”的设备来测量眼睛的位置和运动。“眼动追踪仪”有一个红外线摄像头，但是使用的光频率对眼睛是无害的。', '借助图像识别算法，“眼动追踪仪”可以通过了解头部和眼睛的位置、参与者所看屏幕的距离以及眼动追踪仪的位置很准确地估算我们的凝视点。眼动追踪技术有许多用途。比如，它可以帮助检测疲劳驾驶，支持医疗领域以筛查和培训为目的的应用。它也可以用于游戏、市场营销以及人机交互。', '为什么我们的项目特别关注阅读时的眼动追踪呢？\\n当您阅读这些文字时，眼动追踪仪会跟随您的眼睛在文本上移动。这提供了一些信息，比如您花了多长时间来看这段文字，或者更具体地说，您每个词花了多长时间，您跳过了哪些词，您专注在哪些词上，以及是否需要回头重新阅读某些部分来更好地理解文本。', '当您的大脑在处理文本内容时，您的眼球运动几乎实时反映了大量语言和认知处理过程。因此，记录下来的数据是一座金矿，蕴含着我们如何将文本的含义和语法结构组合在一起的信息。它显示了我们在文本的哪些部分感到费劲以及哪些部分容易读懂。至于是哪些语言因素导致了哪种类型的眼球运动，这就要靠研究人员日后来解释了。', 'MultiplEYE项目的动机是眼动追踪数据相对不足，特别是对于一些较为小众的语言。收集如此庞大的数据是一个挑战，需要在实验设计、复杂程度和参与者阅读的文本类型等方面进行充分的开发和协商。其他看起来可能不太相关，但实际上非常重要的决定包括文本的字体类型、大小，文本的顺序、实验程序以及数据的处理方式。', '然而一旦完成，这个数据集将有助于我们研究心理语言学和计算语言学相关的多个主题，比如我们可以比较不同语言中的阅读行为。举个例子，像拉丁字母、西里尔字母或阿拉伯字母等不同的文字，是否会影响阅读时间？当涉及到文本的计算处理时，我们可以利用眼动追踪数据来推进模仿人类阅读过程的人工智能应用程序，这可以用于构建更好的机器翻译系统或改进从文本中自动提取关键字的功能。', '我们希望通过让许多参与者，包括您自己，阅读许多不同语言的文本来获得眼动数据，为我们的研究打下坚实基础。这将成为我们研究网络成功的主要因素。我们希望通过支持和联结大量研究人员，推进语言学的各个子领域研究。', 'MultiplEYE行动的主要成果将是一个包含多种语言眼动数据的大型数据集，并为基于这种数据的新合作提供平台。如果您正在阅读这段文字，那么您已经在支持我们的事业了，因为您允许我们收集并分析您阅读和理解语言时的眼动数据。非常感谢您的支持！']}\n",
            "en {'stimulus_id': 1, 'stimulus_name': 'PopSci_MultiplEYE', 'stimulus_type': 'experiment', 'pages': ['The MultiplEYE Project\\n\\nThe name \"MultiplEYE\" is a wordplay combining \"multilingualism\" or \"multiple languages\" with \"eye\" from \"eye-tracking\". MultiplEYE is a COST Action funded by the European Union. COST Actions are research networks supported by the European Cooperation in Science and Technology or COST for short. As a funding organisation, COST supports our growing network of researchers across Europe and beyond by providing financial assistance for conducting different networking activities.', 'These activities include working group meetings, training schools to share skills with younger researchers, and scientific research visits. The project title of the MultiplEYE COST Action is: Enabling multilingual eye-tracking data collection for human and machine language processing research. This means that the MultiplEYE COST Action aims to foster an interdisciplinary network of research groups working on collecting eye-tracking data from reading in multiple languages.', 'The goal is to support the development of a large multilingual eye-tracking corpus and enable researchers to collect data by sharing their knowledge between various fields, including linguistics, psychology, speech and language pathology, and computer science. This data collection can then be used to study human language processing from a psycholinguistic perspective as well as to improve and evaluate computational language processing from a machine-learning perspective.', 'What is \"eye-tracking\"?\\nEye-tracking is the process of measuring the point of gaze - where you are looking - and the movements of the eyes between fixed points of gaze. The device used to measure the eye positions and eye movements is called an eye-tracker. It consists of an infrared camera, using a light frequency that does not bother or hurt the human eye.', \"With the help of image recognition algorithms, the eye-tracker can estimate gaze points very accurately by knowing the position of the head and eyes, the distance to the screen a participant is looking at and the eye-tracker's position. Eye-tracking is a helpful technology for many applications. For example, it can help detect tiredness while driving or it can support applications for screening and training purposes in the medical domain. Eye-tracking is also used in gaming, marketing, and human-computer interaction.\", \"Why is eye-tracking while reading especially interesting for our project?\\nAs you read these words, the eye-tracker follows your eye's movements over the text. This provides information about how long you spend looking at a text, or more specifically, how long you spent on each word, which words you skipped, which words you dwelled on, and whether you had to go back and reread parts of the text to understand it better.\", \"As your brain is processing the content of the text, your eye movements reflect a lot of the linguistic and cognitive processing going on almost in real time. Thus, the recorded data is a gold mine of information about how we put together a text's meaning and grammatical structures. It shows which parts of the text we struggle with and which parts are easily readable. It is up to the researchers to later explain which linguistic factors caused which type of eye movements.\", 'The motivation behind MultiplEYE is that eye-tracking data is still sparse, especially for languages with fewer speakers. Such extensive data collection is a challenge in terms of developing and agreeing on the experimental design, the complexity, and the types of texts to be read by the participants. Other decisions that seem less relevant but are, in fact, very important include the font type and size the text is presented in, the order of the texts, the experiment procedure, and how the data will be processed.', 'But once completed, this dataset will allow us to investigate many topics related to psycholinguistics and computational linguistics. For example, we can compare the reading behaviour across different languages. Does the scripture, for example, the Latin alphabet versus Cyrillic or Arabic scripts, impact reading times? An example concerning the computational processing of text could involve using eye-tracking data to advance artificial intelligence applications that imitate the human reading process. This could be used to build better machine translation systems or to improve the automatic extraction of keywords from the text.', 'Receiving eye-tracking data from many participants, including yourself, by reading texts in many different languages will be a great foundation for our research. It will be the main factor in turning our research network into a successful endeavour. We hope to clear the way for advancing research in various subfields of linguistics by supporting and connecting a large group of researchers.', 'The main outcomes of the MultiplEYE Action will be a large dataset containing eye-tracking data in many languages and a platform for new collaborations building on this type of data. If you are reading this text, you are already supporting our cause by allowing us to collect and analyse your eye movements while reading and comprehending language. Thank you!']}\n"
          ]
        }
      ],
      "source": [
        "all_data = load_all_json('languages_json')\n",
        "for k,v in all_data.items():\n",
        "  print(k, v[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5Z0HnuEDdxt"
      },
      "source": [
        "## Prepare spaCy code to generate template csv files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hnvn1bCFWLF_"
      },
      "outputs": [],
      "source": [
        "LANG_FOLDER = \"languages_json\"\n",
        "NLP_MODEL = None\n",
        "CURRENT_LANG = ''\n",
        "IN_DIR = 'languages_json/'\n",
        "\n",
        "from spacy.util import get_lang_class\n",
        "\n",
        "\n",
        "def exists_spacy_blank(lang_code):\n",
        "    try:\n",
        "        get_lang_class(lang_code)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def load_spacy_model(lang_code, small=True):\n",
        "    model = None\n",
        "    if lang_code in SPACY_LANGUAGES:\n",
        "        genre = 'news'\n",
        "        if lang_code in {'zh', 'en'}:\n",
        "            genre = 'web'\n",
        "        if lang_code == 'rm':\n",
        "            return ''\n",
        "        model_name = f'{lang_code}_core_{genre}_{\"sm\" if small else \"lg\"}'\n",
        "        print(f\"Loading model {model_name} for {lang_code}\")\n",
        "        model = spacy.load(model_name)\n",
        "        model.add_pipe(\"sentencizer\")\n",
        "    elif lang_code == \"rm\":\n",
        "        model = spacy.load(\"it_core_news_lg\")\n",
        "        # keep 'morphologizer' ?\n",
        "        model.disable_pipes('tok2vec', 'tagger', 'parser', 'lemmatizer', 'attribute_ruler', 'ner')\n",
        "    elif lang_code == 'gsw':\n",
        "        model = spacy.load('de_core_news_lg')\n",
        "    elif exists_spacy_blank(lang_code):\n",
        "        print(f\"Loading model blank model for {lang_code}\")\n",
        "        model = spacy.blank(lang_code)\n",
        "        model.add_pipe(\"sentencizer\")\n",
        "    else:\n",
        "        model_name = f'xx_sent_ud_sm'\n",
        "        print(f\"Loading model {model_name} for {lang_code}\")\n",
        "        model = spacy.load(model_name)\n",
        "        model.add_pipe(\"sentencizer\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_nlp(lang_code, small=False):\n",
        "    \"\"\"To avoid loading all models at the same time\n",
        "    \"\"\"\n",
        "    global NLP_MODEL, CURRENT_LANG\n",
        "    if lang_code != CURRENT_LANG:\n",
        "        try:\n",
        "            print(f\"Deleting model for {CURRENT_LANG}\")\n",
        "            del NLP_MODEL\n",
        "        except:\n",
        "            print(\"No model to delete\")\n",
        "        print(f\"Loading model for {lang_code}\")\n",
        "        NLP_MODEL = load_spacy_model(lang_code, small=small)\n",
        "        CURRENT_LANG = lang_code\n",
        "    return NLP_MODEL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/multipleye_stimuli_zh_segmentation_pages.zip"
      ],
      "metadata": {
        "id": "VVCtm11zolFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess custom tokens\n",
        "# format should be dict[sname] = [str1, str2, str3, ...] where each index is a different page\n",
        "\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "root = Path(\"/content/multipleye_stimuli_zh_segmentation_pages\")\n",
        "\n",
        "def read_text_safely(fp):\n",
        "    b = fp.read_bytes()\n",
        "    for enc in (\"utf-8\", \"utf-8-sig\", \"cp1252\", \"latin-1\"):\n",
        "        try:\n",
        "            return b.decode(enc)\n",
        "        except UnicodeDecodeError:\n",
        "            pass\n",
        "    return b.decode(\"utf-8\", errors=\"replace\")\n",
        "\n",
        "def key_name(p):\n",
        "    parts = p.stem.split(\"_\")\n",
        "    return \"_\".join(parts[:2]) if len(parts) >= 2 else p.stem\n",
        "\n",
        "page_header = re.compile(r\"^===\\s*PAGE\\s*\\d+\\s*===\\s*$\",\n",
        "                         re.IGNORECASE | re.MULTILINE)\n",
        "\n",
        "def clean_page_text(block):\n",
        "    lines = [ln for ln in block.splitlines() if not page_header.match(ln.strip())]\n",
        "    txt = \" \".join(lines)\n",
        "    txt = txt.replace(\"***\", \" \")\n",
        "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
        "    return txt\n",
        "\n",
        "custom_tokens = {}\n",
        "\n",
        "files = list(root.rglob(\"*_seg_pages.txt\"))\n",
        "if not files:\n",
        "    files = list(root.rglob(\"*.txt\"))\n",
        "\n",
        "for fp in sorted(files):\n",
        "    if fp.name.startswith(\".\"):\n",
        "        continue\n",
        "\n",
        "    k = key_name(fp)\n",
        "    raw = read_text_safely(fp).replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "\n",
        "    first_nonempty = next((ln for ln in raw.splitlines() if ln.strip()), \"\")\n",
        "    if not page_header.match(first_nonempty.strip()):\n",
        "        raw = \"=== PAGE 1 ===\\n\" + raw\n",
        "\n",
        "    blocks = [b for b in re.split(page_header, raw) if b.strip()]\n",
        "    pages = [clean_page_text(b) for b in blocks]\n",
        "\n",
        "    custom_tokens[k] = pages"
      ],
      "metadata": {
        "id": "EbDu2WjQo4Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for item, value in custom_tokens.items():\n",
        "#     for page in value:\n",
        "#         print(item, page)"
      ],
      "metadata": {
        "id": "EtbdR4UTrtHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokens import Doc\n",
        "# outputs content from custom_tokens dict by pages\n",
        "class CustomTokenizer:\n",
        "    def __init__(self, vocab, pages_by_key):\n",
        "        self.vocab = vocab\n",
        "        self.pages_by_key = pages_by_key\n",
        "        self.key = None\n",
        "        self.page_idx = 0\n",
        "\n",
        "    def set_key(self, key):\n",
        "        if key != self.key:\n",
        "            self.key = key\n",
        "            self.page_idx = 0\n",
        "\n",
        "    def __call__(self, text):\n",
        "        pages = self.pages_by_key.get(self.key, [])\n",
        "        words = pages[self.page_idx].split() if self.page_idx < len(pages) else []\n",
        "        self.page_idx += 1\n",
        "        spaces = [True] * len(words)\n",
        "        if spaces: spaces[-1] = False\n",
        "        return Doc(self.vocab, words=words, spaces=spaces)"
      ],
      "metadata": {
        "id": "nWN4y_1eiyjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "13jbiTKgbx4O"
      },
      "outputs": [],
      "source": [
        "def feats_str(token):\n",
        "    if not token.morph:\n",
        "        return \"_\"\n",
        "    md = token.morph.to_dict()\n",
        "    if not md:\n",
        "        return \"_\"\n",
        "    bits = []\n",
        "    for k in sorted(md):\n",
        "        v = md[k]\n",
        "        if isinstance(v, (list, tuple)):\n",
        "            bits.append(f\"{k}={','.join(v)}\")\n",
        "        else:\n",
        "            bits.append(f\"{k}={v}\")\n",
        "    return \"|\".join(bits) if bits else \"_\"\n",
        "\n",
        "\n",
        "def get_head(token, sent):\n",
        "    if token.head == token or token.dep_ == \"ROOT\":\n",
        "        head = 0\n",
        "        deprel = \"root\"\n",
        "    else:\n",
        "        head = (token.head.i - sent.start) + 1  # 1-based in sentence\n",
        "        deprel = token.dep_.lower() if token.dep_ else \"_\"\n",
        "    return head, deprel\n",
        "\n",
        "\n",
        "def get_misc(token, include_ner=True):\n",
        "    misc_parts = []\n",
        "    if not token.whitespace_:\n",
        "        misc_parts.append(\"SpaceAfter=No\")\n",
        "    if include_ner and token.ent_iob_ != \"O\":\n",
        "        misc_parts.append(f\"NER={token.ent_iob_}-{token.ent_type_}\")\n",
        "    misc = \"|\".join(misc_parts) if misc_parts else \"_\"\n",
        "    return misc\n",
        "\n",
        "\n",
        "def iter_pages(stimuli, nlp, set_key=None):\n",
        "    for stim in stimuli:\n",
        "        sid, sname = stim[\"stimulus_id\"], stim[\"stimulus_name\"]\n",
        "        if set_key: set_key(sname)\n",
        "\n",
        "        for pnum, page_text in enumerate(stim[\"pages\"], start=1):\n",
        "            yield sid, sname, pnum, nlp(page_text)\n",
        "\n",
        "def stimuli2csv(stimuli, lang_code, level=\"page\", small=False):\n",
        "    rows = []\n",
        "    nlp = get_nlp(lang_code, small=small)\n",
        "\n",
        "    if lang_code == \"zh\":\n",
        "        zh_tok = CustomTokenizer(nlp.vocab, custom_tokens)\n",
        "        nlp.tokenizer = zh_tok\n",
        "        set_key = zh_tok.set_key\n",
        "    else:\n",
        "        set_key = None\n",
        "\n",
        "    for sid, sname, page, doc in iter_pages(stimuli, nlp, set_key=set_key):\n",
        "        for sent_idx, sentence in enumerate(doc.sents):\n",
        "            eos = {\n",
        "              \"language\": CODE2LANG[lang_code],\n",
        "              \"language_code\": lang_code,\n",
        "              \"stimulus_name\": sname,\n",
        "              \"page\": page,\n",
        "              #\"sent_idx\": sent_idx+1,\n",
        "              \"token\": \"<eos>\",\n",
        "              \"is_alpha\": False,\n",
        "              \"is_stop\": False,\n",
        "              \"is_punct\": False,\n",
        "              \"lemma\": \"\",\n",
        "              \"upos\": \"\",\n",
        "              \"xpos\": \"\",\n",
        "              \"feats\": \"\",\n",
        "              \"head\": \"\",\n",
        "              \"deprel\": \"\",\n",
        "              \"deps\": \"\",\n",
        "              \"misc\": \"\"\n",
        "              }\n",
        "            for token in sentence:\n",
        "                head, deprel = get_head(token, sentence)\n",
        "                rows.append(\n",
        "                    {\n",
        "                        #\"stimulus_id\": sid,\n",
        "                        \"language\": CODE2LANG[lang_code],\n",
        "                        \"language_code\": lang_code,\n",
        "                        \"stimulus_name\": sname,\n",
        "                        \"page\": page,\n",
        "                        #\"sent_idx\": sent_idx+1,\n",
        "                        \"token\": token.text,\n",
        "                        \"is_alpha\": token.is_alpha,\n",
        "                        \"is_stop\": token.is_stop,\n",
        "                        \"is_punct\": token.is_punct,\n",
        "                        \"lemma\": token.lemma_,\n",
        "                        \"upos\": token.pos_,\n",
        "                        \"xpos\": token.tag_,\n",
        "                        \"feats\": feats_str(token),\n",
        "                        \"head\": head,\n",
        "                        \"deprel\": deprel,\n",
        "                        \"deps\": \"_\",\n",
        "                        \"misc\": get_misc(token, include_ner=True)\n",
        "                    }\n",
        "                )\n",
        "            rows.append(eos)\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(rows).sort_values(by=[\"stimulus_name\", \"page\"])\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE4-VqxpOrt_"
      },
      "source": [
        "## Generate csv templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMb69Drkbx7D",
        "outputId": "f4c0beeb-3b09-49f2-9ef3-53a3e2ae9971"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 1/2 [00:01<00:01,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleting model for zh\n",
            "Loading model for en\n",
            "Loading model en_core_web_lg for en\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:06<00:00,  3.41s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "preproc = defaultdict(dict)\n",
        "for lang_code, data in tqdm(all_data.items()):\n",
        "    if lang_code not in LANGS:\n",
        "        continue\n",
        "    preproc[lang_code] = stimuli2csv(data, lang_code, small=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkFLQik9U-sm"
      },
      "source": [
        "## Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "iBjlX2JFURsg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01feab49-c949-4e79-bffd-199d39bcbbbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 14.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/dump/zh/Arg_PISACowsMilk.csv\n",
            "data/dump/zh/Arg_PISARapaNui.csv\n",
            "data/dump/zh/Enc_WikiMoon.csv\n",
            "data/dump/zh/Ins_HumanRights.csv\n",
            "data/dump/zh/Ins_LearningMobility.csv\n",
            "data/dump/zh/Lit_Alchemist.csv\n",
            "data/dump/zh/Lit_BrokenApril.csv\n",
            "data/dump/zh/Lit_MagicMountain.csv\n",
            "data/dump/zh/Lit_NorthWind.csv\n",
            "data/dump/zh/Lit_Solaris.csv\n",
            "data/dump/zh/PopSci_Caveman.csv\n",
            "data/dump/en/Arg_PISACowsMilk.csv\n",
            "data/dump/en/Arg_PISARapaNui.csv\n",
            "data/dump/en/Enc_WikiMoon.csv\n",
            "data/dump/en/Ins_HumanRights.csv\n",
            "data/dump/en/Ins_LearningMobility.csv\n",
            "data/dump/en/Lit_Alchemist.csv\n",
            "data/dump/en/Lit_BrokenApril.csv\n",
            "data/dump/en/Lit_MagicMountain.csv\n",
            "data/dump/en/Lit_NorthWind.csv\n",
            "data/dump/en/Lit_Solaris.csv\n",
            "data/dump/en/PopSci_Caveman.csv\n",
            "data/dump/en/PopSci_MultiplEYE.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "for lang_code, df in tqdm(preproc.items()):\n",
        "    lang_out = 'gsw' if lang_code == 'zd' else lang_code\n",
        "    out_dir = os.path.join(OUT_DIR, lang_out)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    for stim_name, group in df.groupby('stimulus_name'):\n",
        "        out_fis = os.path.join(out_dir, f\"{stim_name}.csv\")\n",
        "        g = group.copy()\n",
        "        g['language_code'] = lang_out\n",
        "        g.to_csv(out_fis, index=False)\n",
        "        print(out_fis)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !zip -r /content/data_dump.zip /content/data/dump\n",
        "# from google.colab import files\n",
        "# files.download('/content/data_dump.zip')"
      ],
      "metadata": {
        "id": "Ta8MZh_fa62N"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}