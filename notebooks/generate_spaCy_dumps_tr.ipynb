{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q17gmm7G6VZX"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "LANGS = [x.strip() for x in os.getenv(\"LANGS\", \"tr\").split(\",\") if x.strip()]\n",
        "OUT_DIR = os.getenv(\"OUT_DIR\", \"data/dump\").strip()\n",
        "\n",
        "print(\"params:\", LANGS, OUT_DIR)\n",
        "os.makedirs(OUT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAFL0X_pKEWD",
        "outputId": "3f1bf646-0b90-4d8f-a65c-2a17b4c94934"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "params: ['tr'] data/dump\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHhXRZOf7aIx"
      },
      "source": [
        "## Get spacy models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGNUWOHg7mKz",
        "outputId": "8ceee76b-6bd8-4aeb-9dd1-c758b09c0c56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xx-sent-ud-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/xx_sent_ud_sm-3.8.0/xx_sent_ud_sm-3.8.0-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('xx_sent_ud_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "import sys, subprocess\n",
        "!python -m spacy download xx_sent_ud_sm\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"spacy==3.4.4\", \"numpy<1.23\"])\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "    \"tr_core_news_lg @ https://huggingface.co/turkish-nlp-suite/tr_core_news_lg/resolve/main/tr_core_news_lg-1.0-py3-none-any.whl\"])\n",
        "language_models = {\n",
        "    \"tr\": \"tr_core_news_lg\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "zAl_1bx6b2Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3J7nrCc7ebN"
      },
      "source": [
        "## Get the MultiplEYE json data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4L0uFaHSk-y"
      },
      "outputs": [],
      "source": [
        "! rm -rf languages*\n",
        "! wget https://github.com/senisioi/repository/releases/download/eyelanguages0/languages_json_all.zip\n",
        "! unzip languages_json_all.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0xc3i_xvQU1Z"
      },
      "outputs": [],
      "source": [
        "SPACY_LANGUAGES = [\"tr\"]\n",
        "\n",
        "CODE2LANG = {\n",
        "    \"tr\": \"Turkish\",\n",
        "}\n",
        "\n",
        "LANGUAGES = list(CODE2LANG.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDPp5lF77uB2"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "R27JzBYj9iIw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import spacy\n",
        "\n",
        "def load_all_json(lang_folder):\n",
        "    all_data = {}\n",
        "    for file in os.listdir(lang_folder):\n",
        "        if file.endswith('.json'):\n",
        "            lang_code = file.replace('.json', '').replace('multipleye_stimuli_experiment_', '')\n",
        "            if lang_code == 'zd':\n",
        "                lang_code = 'gsw'\n",
        "            if (lang_code not in LANGUAGES) or (lang_code not in LANGS):\n",
        "                continue\n",
        "            with open(os.path.join(lang_folder, file), 'r', encoding='utf-8') as f:\n",
        "                all_data[lang_code] = json.load(f)\n",
        "    return all_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqdZjNWw9i6C",
        "outputId": "ac903c37-2056-4813-ae1f-dd2c0e02a383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tr {'stimulus_id': 1, 'stimulus_name': 'PopSci_MultiplEYE', 'stimulus_type': 'experiment', 'pages': ['MultiplEYE Projesi\\n\\n“MultiplEYE” adı, “çok dillilik” veya “çoklu dil” ile “göz izleme”den gelen “göz”ü\\nbirleştiren bir kelime oyunudur. MultiplEYE, Avrupa Birliği tarafından finanse\\nedilen bir COST Aksiyonudur. COST Aksiyonları, Bilim ve Teknolojide Avrupa\\nİşbirliği, kısaca COST, tarafından desteklenen araştırma ağlarıdır. Bir finansman\\nkuruluşu olarak COST, farklı ağ oluşturma faaliyetlerinin yürütülmesi için mali\\nyardım sağlayarak Avrupa ve ötesinde büyüyen araştırmacı ağımızı desteklemektedir.', 'Bu faaliyetler arasında çalışma grubu toplantıları, genç araştırmacılarla beceri\\npaylaşımı için eğitim okulları ve bilimsel araştırma ziyaretleri yer almaktadır.\\nMultiplEYE COST Action’ın proje başlığı: İnsan ve makine dili işleme araştırmaları\\niçin çok dilli göz izleme verilerinin toplanmasının sağlanmasıdır. Bu, MultiplEYE\\nCOST Aksiyonunun çok sayıda dilde okuma göz izleme verilerinin toplanması üzerine\\nçalışan araştırma gruplarından oluşan disiplinler arası bir ağı teşvik etmeyi\\namaçladığı anlamına gelmektedir.', 'Amaç, geniş bir çok dilli göz izleme derleminin geliştirilmesini desteklemek ve\\naraştırmacıların dilbilim, psikoloji, konuşma ve dil patolojisi ve bilgisayar\\nbilimi gibi çeşitli alanlar arasında bilgilerini paylaşarak veri toplamalarını\\nsağlamaktır. Bu veri toplama daha sonra insan dili işlemeyi psikolinguistik bir\\nperspektiften incelemek ve makine öğrenimi perspektifinden hesaplamalı dil\\nişlemeyi geliştirmek ve değerlendirmek için kullanılabilir.', '“Göz izleme” nedir?\\nGöz izleme, bakış noktasını - nereye baktığınızı - ve sabit bakış noktaları\\narasındaki göz hareketlerini ölçme işlemidir. Göz pozisyonlarını ve göz\\nhareketlerini ölçmek için kullanılan cihaza göz izleme cihazı denir. İnsan gözünü\\nrahatsız etmeyen veya zarar vermeyen bir ışık frekansı kullanan bir kızılötesi\\nkameradan oluşur.', 'Görüntü tanıma algoritmalarının yardımıyla, göz izleme cihazı, başın ve gözlerin\\nkonumunu, katılımcının baktığı ekrana olan mesafeyi ve göz izleme cihazının\\nkonumunu bilerek bakış noktalarını çok doğru bir şekilde tahmin edebilir. Göz\\nizleme birçok uygulama için faydalı bir teknolojidir. Örneğin, sürüş sırasında\\nyorgunluğun tespit edilmesine yardımcı olabilir veya tıbbi alanda tarama ve eğitim\\namaçlı uygulamaları destekleyebilir. Göz izleme ayrıca oyun, pazarlama ve\\ninsan-bilgisayar etkileşiminde de kullanılmaktadır.', 'Okurken göz takibi neden projemiz için özellikle ilginç?\\nSiz bu kelimeleri okurken, göz izleme cihazı gözünüzün metin üzerindeki\\nhareketlerini takip etmekte. Bu, bir metne ne kadar süre baktığınız veya daha\\nspesifik olarak, her bir kelimeye ne kadar süre harcadığınız, hangi kelimeleri\\natladığınız, hangi kelimeler üzerinde durduğunuz ve daha iyi anlamak için metnin\\nbazı kısımlarını geri dönüp tekrar okumak zorunda kalıp kalmadığınız hakkında\\nbilgi sağlar.', 'Beyniniz metnin içeriğini işlerken, göz hareketleriniz neredeyse gerçek zamanlı\\nolarak devam eden dilsel ve bilişsel işlemlerin pek çoğunu yansıtır. Bu nedenle,\\nkaydedilen veriler bir metnin anlamını ve gramer yapılarını nasıl bir araya\\ngetirdiğimize dair altın değerinde bir bilgi madenidir. Metnin hangi kısımlarında\\nzorlandığımızı ve hangi kısımlarının kolayca okunabilir olduğunu gösterir. Hangi\\ndilsel faktörlerin hangi tür göz hareketlerine neden olduğunu açıklamak ise daha\\nsonra araştırmacılara kalmaktadır.', 'MultiplEYE’in arkasındaki motivasyon, göz izleme verilerinin özellikle az konuşanı\\nolan diller için hala az olmasıdır. Bu kadar kapsamlı veri toplanması, deney\\ntasarımı ile katılımcıların okuyacağı metinlerin karmaşıklığının ve türlerinin\\nhazırlanması ve üzerinde anlaşmaya varılması açısından zorlu bir süreçtir. Daha az\\nilgili gibi görünen ancak aslında çok önemli olan diğer kararlar arasında metnin\\nhangi yazı tipi ve boyutta sunulacağı, metinlerin sırası, deney prosedürü ve\\nverilerin nasıl işleneceği yer almaktadır.', 'Ancak tamamlandığında, bu veri seti psikodilbilim ve hesaplamalı dilbilim ile\\nilgili birçok konuyu araştırmamıza olanak sağlayacaktır. Örneğin, okuma\\ndavranışını farklı diller arasında karşılaştırabiliriz. Alfabe, örneğin Latin\\nalfabesi, Kiril ya da Arap alfabesi, okuma sürelerini etkiliyor mu? Metnin\\nhesaplamalı olarak işlenmesine ilişkin bir örnek, insan okuma sürecini taklit eden\\nyapay zeka uygulamalarını geliştirmek için göz izleme verilerinin kullanılmasını\\niçerebilir. Bu, daha iyi makine çevirisi sistemleri oluşturmak veya metinden\\nanahtar kelimelerin otomatik olarak çıkarılmasını iyileştirmek için\\nkullanılabilir.', 'Birçok farklı dilde metinler okunurken siz de dahil olmak üzere birçok\\nkatılımcıdan göz izleme verileri almak araştırmamız için büyük bir temel\\noluşturacaktır. Araştırma ağımızın başarılı bir girişime dönüşmesinde ana etken\\nolacaktır. Geniş bir araştırmacı grubunu destekleyerek ve birbirine bağlayarak\\ndilbilimin çeşitli alt alanlarındaki araştırmaları ilerletmenin yolunu açmayı\\numuyoruz.', 'MultiplEYE Aksiyonunun ana çıktıları, birçok dilde göz izleme verilerini içeren\\nbüyük bir veri kümesi ve bu tür veriler üzerinden inşa edilecek yeni işbirlikleri\\niçin bir platform olacaktır. Bu metni okuyorsanız, dili okurken ve anlarken göz\\nhareketlerinizi toplamamıza ve analiz etmemize izin vererek zaten amacımızı\\ndestekliyorsunuz demektir. Teşekkür ederiz!']}\n"
          ]
        }
      ],
      "source": [
        "all_data = load_all_json('languages_json')\n",
        "for k,v in all_data.items():\n",
        "  print(k, v[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5Z0HnuEDdxt"
      },
      "source": [
        "## Prepare spaCy code to generate template csv files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Hnvn1bCFWLF_"
      },
      "outputs": [],
      "source": [
        "LANG_FOLDER = \"languages_json\"\n",
        "NLP_MODEL = None\n",
        "CURRENT_LANG = ''\n",
        "IN_DIR = 'languages_json/'\n",
        "\n",
        "from spacy.util import get_lang_class\n",
        "\n",
        "\n",
        "def exists_spacy_blank(lang_code):\n",
        "    try:\n",
        "        get_lang_class(lang_code)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def load_spacy_model(lang_code, small=True):\n",
        "    model = None\n",
        "    if lang_code in SPACY_LANGUAGES:\n",
        "        genre = 'news'\n",
        "        if lang_code in {'zh', 'en'}:\n",
        "            genre = 'web'\n",
        "        if lang_code == 'rm':\n",
        "            return ''\n",
        "        model_name = f'{lang_code}_core_{genre}_{\"sm\" if small else \"lg\"}'\n",
        "        print(f\"Loading model {model_name} for {lang_code}\")\n",
        "        model = spacy.load(model_name)\n",
        "        model.add_pipe(\"sentencizer\")\n",
        "    elif lang_code == \"rm\":\n",
        "        model = spacy.load(\"it_core_news_lg\")\n",
        "        # keep 'morphologizer' ?\n",
        "        model.disable_pipes('tok2vec', 'tagger', 'parser', 'lemmatizer', 'attribute_ruler', 'ner')\n",
        "    elif lang_code == 'gsw':\n",
        "        model = spacy.load('de_core_news_lg')\n",
        "    elif exists_spacy_blank(lang_code):\n",
        "        print(f\"Loading model blank model for {lang_code}\")\n",
        "        model = spacy.blank(lang_code)\n",
        "        model.add_pipe(\"sentencizer\")\n",
        "    else:\n",
        "        model_name = f'xx_sent_ud_sm'\n",
        "        print(f\"Loading model {model_name} for {lang_code}\")\n",
        "        model = spacy.load(model_name)\n",
        "        model.add_pipe(\"sentencizer\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_nlp(lang_code, small=False):\n",
        "    \"\"\"To avoid loading all models at the same time\n",
        "    \"\"\"\n",
        "    global NLP_MODEL, CURRENT_LANG\n",
        "    if lang_code != CURRENT_LANG:\n",
        "        try:\n",
        "            print(f\"Deleting model for {CURRENT_LANG}\")\n",
        "            del NLP_MODEL\n",
        "        except:\n",
        "            print(\"No model to delete\")\n",
        "        print(f\"Loading model for {lang_code}\")\n",
        "        NLP_MODEL = load_spacy_model(lang_code, small=small)\n",
        "        CURRENT_LANG = lang_code\n",
        "    return NLP_MODEL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "13jbiTKgbx4O"
      },
      "outputs": [],
      "source": [
        "def feats_str(token):\n",
        "    if not token.morph:\n",
        "        return \"_\"\n",
        "    md = token.morph.to_dict()\n",
        "    if not md:\n",
        "        return \"_\"\n",
        "    bits = []\n",
        "    for k in sorted(md):\n",
        "        v = md[k]\n",
        "        if isinstance(v, (list, tuple)):\n",
        "            bits.append(f\"{k}={','.join(v)}\")\n",
        "        else:\n",
        "            bits.append(f\"{k}={v}\")\n",
        "    return \"|\".join(bits) if bits else \"_\"\n",
        "\n",
        "\n",
        "def get_head(token, sent):\n",
        "    if token.head == token or token.dep_ == \"ROOT\":\n",
        "        head = 0\n",
        "        deprel = \"root\"\n",
        "    else:\n",
        "        head = (token.head.i - sent.start) + 1  # 1-based in sentence\n",
        "        deprel = token.dep_.lower() if token.dep_ else \"_\"\n",
        "    return head, deprel\n",
        "\n",
        "\n",
        "def get_misc(token, include_ner=True):\n",
        "    misc_parts = []\n",
        "    if not token.whitespace_:\n",
        "        misc_parts.append(\"SpaceAfter=No\")\n",
        "    if include_ner and token.ent_iob_ != \"O\":\n",
        "        misc_parts.append(f\"NER={token.ent_iob_}-{token.ent_type_}\")\n",
        "    misc = \"|\".join(misc_parts) if misc_parts else \"_\"\n",
        "    return misc\n",
        "\n",
        "\n",
        "def iter_pages(stimuli, nlp):\n",
        "    for stim in stimuli:\n",
        "        sid, sname = stim[\"stimulus_id\"], stim[\"stimulus_name\"]\n",
        "        for pnum, page_text in enumerate(stim[\"pages\"], start=1):\n",
        "            yield sid, sname, pnum, nlp(page_text)\n",
        "\n",
        "def stimuli2csv(stimuli, lang_code, level=\"page\", small=False):\n",
        "    rows = []\n",
        "    nlp = get_nlp(lang_code, small=small)\n",
        "    for sid, sname, page, doc in iter_pages(stimuli, nlp):\n",
        "        ptext = doc.text\n",
        "        document = nlp(ptext)\n",
        "        for sent_idx, sentence in enumerate(document.sents):\n",
        "            eos = {\n",
        "              \"language\": CODE2LANG[lang_code],\n",
        "              \"language_code\": lang_code,\n",
        "              \"stimulus_name\": sname,\n",
        "              \"page\": page,\n",
        "              #\"sent_idx\": sent_idx+1,\n",
        "              \"token\": \"<eos>\",\n",
        "              \"is_alpha\": False,\n",
        "              \"is_stop\": False,\n",
        "              \"is_punct\": False,\n",
        "              \"lemma\": \"\",\n",
        "              \"upos\": \"\",\n",
        "              \"xpos\": \"\",\n",
        "              \"feats\": \"\",\n",
        "              \"head\": \"\",\n",
        "              \"deprel\": \"\",\n",
        "              \"deps\": \"\",\n",
        "              \"misc\": \"\"\n",
        "              }\n",
        "            for token in sentence:\n",
        "                head, deprel = get_head(token, sentence)\n",
        "                rows.append(\n",
        "                    {\n",
        "                        #\"stimulus_id\": sid,\n",
        "                        \"language\": CODE2LANG[lang_code],\n",
        "                        \"language_code\": lang_code,\n",
        "                        \"stimulus_name\": sname,\n",
        "                        \"page\": page,\n",
        "                        #\"sent_idx\": sent_idx+1,\n",
        "                        \"token\": token.text,\n",
        "                        \"is_alpha\": token.is_alpha,\n",
        "                        \"is_stop\": token.is_stop,\n",
        "                        \"is_punct\": token.is_punct,\n",
        "                        \"lemma\": token.lemma_,\n",
        "                        \"upos\": token.pos_,\n",
        "                        \"xpos\": token.tag_,\n",
        "                        \"feats\": feats_str(token),\n",
        "                        \"head\": head,\n",
        "                        \"deprel\": deprel,\n",
        "                        \"deps\": \"_\",\n",
        "                        \"misc\": get_misc(token, include_ner=True)\n",
        "                    }\n",
        "                )\n",
        "            rows.append(eos)\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(rows).sort_values(by=[\"stimulus_name\", \"page\"])\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "preproc = defaultdict(dict)\n",
        "for lang_code, data in tqdm(all_data.items()):\n",
        "    if lang_code not in LANGS:\n",
        "        continue\n",
        "    preproc[lang_code] = stimuli2csv(data, lang_code, small=False)"
      ],
      "metadata": {
        "id": "g4rfEyywMoRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE4-VqxpOrt_"
      },
      "source": [
        "## Generate csv templates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkFLQik9U-sm"
      },
      "source": [
        "## Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBjlX2JFURsg",
        "outputId": "0cb46530-0188-4b42-a9ce-c5630fd42b09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 13.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/dump/ro/Arg_PISACowsMilk.csv\n",
            "data/dump/ro/Arg_PISARapaNui.csv\n",
            "data/dump/ro/Enc_WikiMoon.csv\n",
            "data/dump/ro/Ins_HumanRights.csv\n",
            "data/dump/ro/Ins_LearningMobility.csv\n",
            "data/dump/ro/Lit_Alchemist.csv\n",
            "data/dump/ro/Lit_BrokenApril.csv\n",
            "data/dump/ro/Lit_MagicMountain.csv\n",
            "data/dump/ro/Lit_NorthWind.csv\n",
            "data/dump/ro/Lit_Solaris.csv\n",
            "data/dump/ro/PopSci_Caveman.csv\n",
            "data/dump/ro/PopSci_MultiplEYE.csv\n",
            "data/dump/en/Arg_PISACowsMilk.csv\n",
            "data/dump/en/Arg_PISARapaNui.csv\n",
            "data/dump/en/Enc_WikiMoon.csv\n",
            "data/dump/en/Ins_HumanRights.csv\n",
            "data/dump/en/Ins_LearningMobility.csv\n",
            "data/dump/en/Lit_Alchemist.csv\n",
            "data/dump/en/Lit_BrokenApril.csv\n",
            "data/dump/en/Lit_MagicMountain.csv\n",
            "data/dump/en/Lit_NorthWind.csv\n",
            "data/dump/en/Lit_Solaris.csv\n",
            "data/dump/en/PopSci_Caveman.csv\n",
            "data/dump/en/PopSci_MultiplEYE.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "for lang_code, df in tqdm(preproc.items()):\n",
        "    lang_out = 'gsw' if lang_code == 'zd' else lang_code\n",
        "    out_dir = os.path.join(OUT_DIR, lang_out)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    for stim_name, group in df.groupby('stimulus_name'):\n",
        "        out_fis = os.path.join(out_dir, f\"{stim_name}.csv\")\n",
        "        g = group.copy()\n",
        "        g['language_code'] = lang_out\n",
        "        g.to_csv(out_fis, index=False)\n",
        "        print(out_fis)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}