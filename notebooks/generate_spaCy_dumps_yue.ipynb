{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q17gmm7G6VZX"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAf08EAe6WPg",
        "outputId": "987705da-7e4e-47b1-8937-520c294a286b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "params: ['yue'] data/dump\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "LANGS = [x.strip() for x in os.getenv(\"LANGS\", \"yue\").split(\",\") if x.strip()]\n",
        "OUT_DIR = os.getenv(\"OUT_DIR\", \"data/dump\").strip()\n",
        "\n",
        "print(\"params:\", LANGS, OUT_DIR)\n",
        "os.makedirs(OUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHhXRZOf7aIx"
      },
      "source": [
        "## Get spacy models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "tGNUWOHg7mKz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee9cc3e1-9f7d-4f54-ade1-71156666a68d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xx-sent-ud-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/xx_sent_ud_sm-3.8.0/xx_sent_ud_sm-3.8.0-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('xx_sent_ud_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "use blank\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download xx_sent_ud_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3J7nrCc7ebN"
      },
      "source": [
        "## Get the MultiplEYE json data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4L0uFaHSk-y"
      },
      "outputs": [],
      "source": [
        "! rm -rf languages*\n",
        "! wget https://github.com/senisioi/repository/releases/download/eyelanguages0/languages_json_all.zip\n",
        "! unzip languages_json_all.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "0xc3i_xvQU1Z"
      },
      "outputs": [],
      "source": [
        "SPACY_LANGUAGES = [\"ca\", \"de\", \"el\", \"en\", \"es\", \"fr\", \"hr\", \"it\", \"lt\", \"mk\", \"nl\", \"pl\", \"pt\", \"ro\", \"ru\", \"sl\", \"sv\", \"uk\", \"zh\"]\n",
        "\n",
        "CODE2LANG = {\n",
        "    \"yue\": \"Cantonese\"\n",
        "}\n",
        "\n",
        "LANGUAGES = list(CODE2LANG.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDPp5lF77uB2"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "R27JzBYj9iIw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import spacy\n",
        "\n",
        "def load_all_json(lang_folder):\n",
        "    all_data = {}\n",
        "    for file in os.listdir(lang_folder):\n",
        "        if file.endswith('.json'):\n",
        "            lang_code = file.replace('.json', '').replace('multipleye_stimuli_experiment_', '')\n",
        "            if lang_code == 'zd':\n",
        "                lang_code = 'gsw'\n",
        "            if (lang_code not in LANGUAGES) or (lang_code not in LANGS):\n",
        "                continue\n",
        "            with open(os.path.join(lang_folder, file), 'r', encoding='utf-8') as f:\n",
        "                all_data[lang_code] = json.load(f)\n",
        "    return all_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqdZjNWw9i6C",
        "outputId": "0b9c8a34-4ff3-493b-f410-abce236c07cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yue {'stimulus_id': 1, 'stimulus_name': 'PopSci_MultiplEYE', 'stimulus_type': 'experiment', 'pages': ['MultiplEYE项目\\n\\n「MultiplEYE」這個名字玩了一些文字遊戲，巧妙結合了「多語言性（multilingualism / multiple languages）」與「眼動追蹤（eye-tracking)」中的「眼睛（eye）」。MultiplEYE 是由歐盟資助的 COST 行動項目。COST行動項目是由COST（歐洲科學與技術合作組織）專門支持的研究網絡。作為資助機構，COST為歐洲及其他地區的研究人員提供財務支持，以推動不同形式的合作活動。', 'MultiplEYE 行動的正式項目名稱為：促進多語言眼動數據收集，以支持人類與機器語言處理研究。簡言之，MultiplEYE 致力於建立一個跨學科研究網絡，推動各語言研究小組合作，收集閱讀過程中的眼動數據。', '其目標是建立一個大型的多語言眼動語料庫，並透過各學科（如語言學、心理學、言語及語言治療、計算機科學等）之間的知識交流，助力研究員收集數據。這些數據既可以用於從心裡語言學角度分析人類語言處理，也可以用於從機器學習角度，評估與提高計算機語言處理能力。', '那麼，什麼是「眼動追蹤」？\\n眼動追蹤（eye-tracking）是一个測量眼睛注視點（也就是你在看哪裡），以及眼睛在注視點間如何移動的過程。用來測量眼睛註視位置和移動軌跡的裝置被稱為「眼動儀」。它包含一個紅外線攝像頭，以不對人眼造成幹擾及傷害的光頻率進行追蹤。', '藉助圖像識別算法，眼動儀能根據被試頭和眼睛的位置、參與者與屏幕的距離，以及眼動儀本身的位置，精準預測眼睛的註視點。眼動追蹤技術可應用於多個領域。比如，它可以幫助檢測疲勞駕駛、還可以為醫療領域診斷與訓練的應用提供支持。它還可以用於遊戲設計、市場營銷、以及人機互動等。', '為什麼我們的項目特別關註閱讀時的眼動追蹤？\\n當你閱讀這段文字時，眼動儀會追蹤你的眼睛在文字上的移動。這也提供了相關的信息，包括你花了多長時間來看這段文本，在每個詞上花多長時間，跳過了哪些詞，專注於哪些詞，以及是否需要回頭重新閱讀某些部分，以更好地理解文本。', '當你的大腦在處理文本內容時，你的眼球運動就幾乎實時反映了大量的語言和認知處理過程。因此，記錄下來的數據則是一座關於我們如何組織文本內容、處理語法結構的信息金礦。它體現了我們在哪些部分更糾結，哪些部分更好懂。至於是哪些語言因素導致了哪類眼球運動，就要靠研究人員日後進行解釋了。', 'MultiplEYE 項目的研究動機，始於依舊不足的眼動追蹤數據。尤其對於一些使用者少的語言來說，這樣的數據更為稀有。大規模的眼動數據收集極具挑戰，包括如何設計實驗、實驗復雜程度、以及被試需要閱讀的文本類型。事實上，還有一些其它看起來不相關、實際上非常重要的因素需要決定，包括文本字體類型、大小，文本閱讀順序，實驗流程及數據如何處理等。', '然而一旦完成，這個數據庫將有助於我們對心裡語言學和計算語言學的眾多議題展開研究。例如，我們可以比較不同語言中的閱讀行為。舉個例子，像拉丁字母、斯拉夫字母或阿拉伯字母等不同的文字，是否會影響各自的閱讀時間？而在涉及到文本的計算處理時，我們則可以利用眼動追蹤數據，對模仿人類閱讀過程的人工智能軟件進行性能上的提升。這也可以用於構建更好的機器翻譯係統，並提升文本關鍵詞自動提取的功能。', '我們希望通過讓許多被試（包括您自己）對許多不同語言的文本進行閱讀，從而獲得眼動數據，為我們的研究打下堅實基礎。這將成為助推我們研究網絡成功的一大主要因素。我們希望支持和促進研究人員們的溝通，為多領域的前沿研究掃清障礙，開闢前路。', 'MultiplEYE 行動的主要成果將會是一個多語言的眼動數據集，並基於這一平台展開新的合作。若你正在閱讀這段文字，代表你已在實際支持我們的研究工作，允許我們在你閱讀和理解語言時收集和分析你的眼動數據。非常感謝你的支持！']}\n"
          ]
        }
      ],
      "source": [
        "all_data = load_all_json('languages_json')\n",
        "for k,v in all_data.items():\n",
        "  print(k, v[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5Z0HnuEDdxt"
      },
      "source": [
        "## Prepare spaCy code to generate template csv files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "Hnvn1bCFWLF_"
      },
      "outputs": [],
      "source": [
        "LANG_FOLDER = \"languages_json\"\n",
        "NLP_MODEL = None\n",
        "CURRENT_LANG = ''\n",
        "IN_DIR = 'languages_json/'\n",
        "\n",
        "from spacy.util import get_lang_class\n",
        "\n",
        "\n",
        "def exists_spacy_blank(lang_code):\n",
        "    try:\n",
        "        get_lang_class(lang_code)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def load_spacy_model(lang_code, small=True):\n",
        "    model = None\n",
        "    if lang_code in SPACY_LANGUAGES:\n",
        "        genre = 'news'\n",
        "        if lang_code in {'zh', 'en'}:\n",
        "            genre = 'web'\n",
        "        if lang_code == 'rm':\n",
        "            return ''\n",
        "        model_name = f'{lang_code}_core_{genre}_{\"sm\" if small else \"lg\"}'\n",
        "        print(f\"Loading model {model_name} for {lang_code}\")\n",
        "        model = spacy.load(model_name)\n",
        "        model.add_pipe(\"sentencizer\")\n",
        "    elif lang_code == \"rm\":\n",
        "        model = spacy.load(\"it_core_news_lg\")\n",
        "        # keep 'morphologizer' ?\n",
        "        model.disable_pipes('tok2vec', 'tagger', 'parser', 'lemmatizer', 'attribute_ruler', 'ner')\n",
        "    elif lang_code == 'gsw':\n",
        "        model = spacy.load('de_core_news_lg')\n",
        "    elif exists_spacy_blank(lang_code):\n",
        "        print(f\"Loading model blank model for {lang_code}\")\n",
        "        model = spacy.blank(lang_code)\n",
        "        model.add_pipe(\"sentencizer\")\n",
        "    else:\n",
        "        model_name = f'xx_sent_ud_sm'\n",
        "        print(f\"Loading model {model_name} for {lang_code}\")\n",
        "        model = spacy.load(model_name)\n",
        "        model.add_pipe(\"sentencizer\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_nlp(lang_code, small=False):\n",
        "    \"\"\"To avoid loading all models at the same time\n",
        "    \"\"\"\n",
        "    global NLP_MODEL, CURRENT_LANG\n",
        "    if lang_code != CURRENT_LANG:\n",
        "        try:\n",
        "            print(f\"Deleting model for {CURRENT_LANG}\")\n",
        "            del NLP_MODEL\n",
        "        except:\n",
        "            print(\"No model to delete\")\n",
        "        print(f\"Loading model for {lang_code}\")\n",
        "        NLP_MODEL = load_spacy_model(lang_code, small=small)\n",
        "        CURRENT_LANG = lang_code\n",
        "    return NLP_MODEL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/multipleye_stimuli_yu_segmentation_pages.zip"
      ],
      "metadata": {
        "id": "VVCtm11zolFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess custom tokens\n",
        "# format should be dict[sname] = [str1, str2, str3, ...] where each index is a different page\n",
        "\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "root = Path(\"/content/multipleye_stimuli_yu_segmentation_pages\")\n",
        "\n",
        "def read_text_safely(fp):\n",
        "    b = fp.read_bytes()\n",
        "    for enc in (\"utf-8\", \"utf-8-sig\", \"cp1252\", \"latin-1\"):\n",
        "        try:\n",
        "            return b.decode(enc)\n",
        "        except UnicodeDecodeError:\n",
        "            pass\n",
        "    return b.decode(\"utf-8\", errors=\"replace\")\n",
        "\n",
        "def key_name(p):\n",
        "    parts = p.stem.split(\"_\")\n",
        "    return \"_\".join(parts[:2]) if len(parts) >= 2 else p.stem\n",
        "\n",
        "page_header = re.compile(r\"^===\\s*PAGE\\s*\\d+\\s*===\\s*$\",\n",
        "                         re.IGNORECASE | re.MULTILINE)\n",
        "\n",
        "def clean_page_text(block):\n",
        "    lines = [ln for ln in block.splitlines() if not page_header.match(ln.strip())]\n",
        "    txt = \" \".join(lines)\n",
        "    txt = txt.replace(\"***\", \" \")\n",
        "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
        "    return txt\n",
        "\n",
        "custom_tokens = {}\n",
        "\n",
        "files = list(root.rglob(\"*_seg_pages.txt\"))\n",
        "if not files:\n",
        "    files = list(root.rglob(\"*.txt\"))\n",
        "\n",
        "for fp in sorted(files):\n",
        "    if fp.name.startswith(\".\"):\n",
        "        continue\n",
        "\n",
        "    k = key_name(fp)\n",
        "    raw = read_text_safely(fp).replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "\n",
        "    first_nonempty = next((ln for ln in raw.splitlines() if ln.strip()), \"\")\n",
        "    if not page_header.match(first_nonempty.strip()):\n",
        "        raw = \"=== PAGE 1 ===\\n\" + raw\n",
        "\n",
        "    blocks = [b for b in re.split(page_header, raw) if b.strip()]\n",
        "    pages = [clean_page_text(b) for b in blocks]\n",
        "\n",
        "    custom_tokens[k] = pages"
      ],
      "metadata": {
        "id": "EbDu2WjQo4Lh"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for item, value in custom_tokens.items():\n",
        "#     for page in value:\n",
        "#         print(item, page)"
      ],
      "metadata": {
        "id": "EtbdR4UTrtHi"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokens import Doc\n",
        "# outputs content from custom_tokens dict by pages\n",
        "class CustomTokenizer:\n",
        "    def __init__(self, vocab, pages_by_key):\n",
        "        self.vocab = vocab\n",
        "        self.pages_by_key = pages_by_key\n",
        "        self.key = None\n",
        "        self.page_idx = 0\n",
        "\n",
        "    def set_key(self, key):\n",
        "        if key != self.key:\n",
        "            self.key = key\n",
        "            self.page_idx = 0\n",
        "\n",
        "    def __call__(self, text):\n",
        "        pages = self.pages_by_key.get(self.key, [])\n",
        "        words = pages[self.page_idx].split() if self.page_idx < len(pages) else []\n",
        "        self.page_idx += 1\n",
        "        spaces = [True] * len(words)\n",
        "        if spaces: spaces[-1] = False\n",
        "        return Doc(self.vocab, words=words, spaces=spaces)"
      ],
      "metadata": {
        "id": "nWN4y_1eiyjF"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "13jbiTKgbx4O"
      },
      "outputs": [],
      "source": [
        "def feats_str(token):\n",
        "    if not token.morph:\n",
        "        return \"_\"\n",
        "    md = token.morph.to_dict()\n",
        "    if not md:\n",
        "        return \"_\"\n",
        "    bits = []\n",
        "    for k in sorted(md):\n",
        "        v = md[k]\n",
        "        if isinstance(v, (list, tuple)):\n",
        "            bits.append(f\"{k}={','.join(v)}\")\n",
        "        else:\n",
        "            bits.append(f\"{k}={v}\")\n",
        "    return \"|\".join(bits) if bits else \"_\"\n",
        "\n",
        "\n",
        "def get_head(token, sent):\n",
        "    if token.head == token or token.dep_ == \"ROOT\":\n",
        "        head = 0\n",
        "        deprel = \"root\"\n",
        "    else:\n",
        "        head = (token.head.i - sent.start) + 1  # 1-based in sentence\n",
        "        deprel = token.dep_.lower() if token.dep_ else \"_\"\n",
        "    return head, deprel\n",
        "\n",
        "\n",
        "def get_misc(token, include_ner=True):\n",
        "    misc_parts = []\n",
        "    if not token.whitespace_:\n",
        "        misc_parts.append(\"SpaceAfter=No\")\n",
        "    if include_ner and token.ent_iob_ != \"O\":\n",
        "        misc_parts.append(f\"NER={token.ent_iob_}-{token.ent_type_}\")\n",
        "    misc = \"|\".join(misc_parts) if misc_parts else \"_\"\n",
        "    return misc\n",
        "\n",
        "\n",
        "def iter_pages(stimuli, nlp, set_key=None):\n",
        "    for stim in stimuli:\n",
        "        sid, sname = stim[\"stimulus_id\"], stim[\"stimulus_name\"]\n",
        "        if set_key: set_key(sname)\n",
        "\n",
        "        for pnum, page_text in enumerate(stim[\"pages\"], start=1):\n",
        "            yield sid, sname, pnum, nlp(page_text)\n",
        "\n",
        "def stimuli2csv(stimuli, lang_code, level=\"page\", small=False):\n",
        "    rows = []\n",
        "    nlp = get_nlp(lang_code, small=small)\n",
        "\n",
        "    if lang_code == \"yue\":\n",
        "        yue_tok = CustomTokenizer(nlp.vocab, custom_tokens)\n",
        "        nlp.tokenizer = yue_tok\n",
        "        set_key = yue_tok.set_key\n",
        "    else:\n",
        "        set_key = None\n",
        "\n",
        "    for sid, sname, page, doc in iter_pages(stimuli, nlp, set_key=set_key):\n",
        "        for sent_idx, sentence in enumerate(doc.sents):\n",
        "            eos = {\n",
        "              \"language\": CODE2LANG[lang_code],\n",
        "              \"language_code\": lang_code,\n",
        "              \"stimulus_name\": sname,\n",
        "              \"page\": page,\n",
        "              #\"sent_idx\": sent_idx+1,\n",
        "              \"token\": \"<eos>\",\n",
        "              \"is_alpha\": False,\n",
        "              \"is_stop\": False,\n",
        "              \"is_punct\": False,\n",
        "              \"lemma\": \"\",\n",
        "              \"upos\": \"\",\n",
        "              \"xpos\": \"\",\n",
        "              \"feats\": \"\",\n",
        "              \"head\": \"\",\n",
        "              \"deprel\": \"\",\n",
        "              \"deps\": \"\",\n",
        "              \"misc\": \"\"\n",
        "              }\n",
        "            for token in sentence:\n",
        "                head, deprel = get_head(token, sentence)\n",
        "                rows.append(\n",
        "                    {\n",
        "                        #\"stimulus_id\": sid,\n",
        "                        \"language\": CODE2LANG[lang_code],\n",
        "                        \"language_code\": lang_code,\n",
        "                        \"stimulus_name\": sname,\n",
        "                        \"page\": page,\n",
        "                        #\"sent_idx\": sent_idx+1,\n",
        "                        \"token\": token.text,\n",
        "                        \"is_alpha\": token.is_alpha,\n",
        "                        \"is_stop\": token.is_stop,\n",
        "                        \"is_punct\": token.is_punct,\n",
        "                        \"lemma\": token.lemma_,\n",
        "                        \"upos\": token.pos_,\n",
        "                        \"xpos\": token.tag_,\n",
        "                        \"feats\": feats_str(token),\n",
        "                        \"head\": head,\n",
        "                        \"deprel\": deprel,\n",
        "                        \"deps\": \"_\",\n",
        "                        \"misc\": get_misc(token, include_ner=True)\n",
        "                    }\n",
        "                )\n",
        "            rows.append(eos)\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(rows).sort_values(by=[\"stimulus_name\", \"page\"])\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE4-VqxpOrt_"
      },
      "source": [
        "## Generate csv templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMb69Drkbx7D",
        "outputId": "356c9cd2-63d4-4190-cd0d-e3ee234f88ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  4.42it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "preproc = defaultdict(dict)\n",
        "for lang_code, data in tqdm(all_data.items()):\n",
        "    if lang_code not in LANGS:\n",
        "        continue\n",
        "    preproc[lang_code] = stimuli2csv(data, lang_code, small=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkFLQik9U-sm"
      },
      "source": [
        "## Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "iBjlX2JFURsg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc3290e6-c32d-4013-f49b-19ae5b96420c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 18.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/dump/yue/Arg_PISACowsMilk.csv\n",
            "data/dump/yue/Arg_PISARapaNui.csv\n",
            "data/dump/yue/Enc_WikiMoon.csv\n",
            "data/dump/yue/Ins_HumanRights.csv\n",
            "data/dump/yue/Ins_LearningMobility.csv\n",
            "data/dump/yue/Lit_Alchemist.csv\n",
            "data/dump/yue/Lit_BrokenApril.csv\n",
            "data/dump/yue/Lit_MagicMountain.csv\n",
            "data/dump/yue/Lit_Solaris.csv\n",
            "data/dump/yue/PopSci_Caveman.csv\n",
            "data/dump/yue/PopSci_MultiplEYE.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "for lang_code, df in tqdm(preproc.items()):\n",
        "    lang_out = 'gsw' if lang_code == 'zd' else lang_code\n",
        "    out_dir = os.path.join(OUT_DIR, lang_out)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    for stim_name, group in df.groupby('stimulus_name'):\n",
        "        out_fis = os.path.join(out_dir, f\"{stim_name}.csv\")\n",
        "        g = group.copy()\n",
        "        g['language_code'] = lang_out\n",
        "        g.to_csv(out_fis, index=False)\n",
        "        print(out_fis)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/data_dump.zip /content/data/dump\n",
        "from google.colab import files\n",
        "files.download('/content/data_dump.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "id": "Ta8MZh_fa62N",
        "outputId": "53494899-22d8-4fed-ec25-fa0203d3b351"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/data/dump/ (stored 0%)\n",
            "updating: content/data/dump/zh/ (stored 0%)\n",
            "updating: content/data/dump/zh/Arg_PISACowsMilk.csv (deflated 87%)\n",
            "updating: content/data/dump/zh/Lit_MagicMountain.csv (deflated 87%)\n",
            "updating: content/data/dump/zh/Lit_BrokenApril.csv (deflated 87%)\n",
            "updating: content/data/dump/zh/Lit_Alchemist.csv (deflated 86%)\n",
            "updating: content/data/dump/zh/Ins_HumanRights.csv (deflated 86%)\n",
            "updating: content/data/dump/zh/Enc_WikiMoon.csv (deflated 82%)\n",
            "updating: content/data/dump/zh/PopSci_Caveman.csv (deflated 86%)\n",
            "updating: content/data/dump/zh/Lit_Solaris.csv (deflated 87%)\n",
            "updating: content/data/dump/zh/PopSci_MultiplEYE.csv (deflated 91%)\n",
            "updating: content/data/dump/zh/Lit_NorthWind.csv (deflated 82%)\n",
            "updating: content/data/dump/zh/Ins_LearningMobility.csv (deflated 87%)\n",
            "updating: content/data/dump/zh/Arg_PISARapaNui.csv (deflated 87%)\n",
            "updating: content/data/dump/en/ (stored 0%)\n",
            "updating: content/data/dump/en/Arg_PISACowsMilk.csv (deflated 89%)\n",
            "updating: content/data/dump/en/Lit_MagicMountain.csv (deflated 89%)\n",
            "updating: content/data/dump/en/Lit_BrokenApril.csv (deflated 89%)\n",
            "updating: content/data/dump/en/Lit_Alchemist.csv (deflated 89%)\n",
            "updating: content/data/dump/en/Ins_HumanRights.csv (deflated 88%)\n",
            "updating: content/data/dump/en/Enc_WikiMoon.csv (deflated 84%)\n",
            "updating: content/data/dump/en/PopSci_Caveman.csv (deflated 87%)\n",
            "updating: content/data/dump/en/Lit_Solaris.csv (deflated 89%)\n",
            "updating: content/data/dump/en/PopSci_MultiplEYE.csv (deflated 90%)\n",
            "updating: content/data/dump/en/Lit_NorthWind.csv (deflated 85%)\n",
            "updating: content/data/dump/en/Ins_LearningMobility.csv (deflated 89%)\n",
            "updating: content/data/dump/en/Arg_PISARapaNui.csv (deflated 89%)\n",
            "updating: content/data/dump/yue/ (stored 0%)\n",
            "updating: content/data/dump/yue/Arg_PISACowsMilk.csv (deflated 92%)\n",
            "updating: content/data/dump/yue/Lit_MagicMountain.csv (deflated 93%)\n",
            "updating: content/data/dump/yue/Lit_BrokenApril.csv (deflated 92%)\n",
            "updating: content/data/dump/yue/Lit_Alchemist.csv (deflated 93%)\n",
            "updating: content/data/dump/yue/Ins_HumanRights.csv (deflated 92%)\n",
            "updating: content/data/dump/yue/Enc_WikiMoon.csv (deflated 89%)\n",
            "updating: content/data/dump/yue/PopSci_Caveman.csv (deflated 91%)\n",
            "updating: content/data/dump/yue/Lit_Solaris.csv (deflated 92%)\n",
            "updating: content/data/dump/yue/PopSci_MultiplEYE.csv (deflated 93%)\n",
            "updating: content/data/dump/yue/Lit_NorthWind.csv (deflated 73%)\n",
            "updating: content/data/dump/yue/Ins_LearningMobility.csv (deflated 93%)\n",
            "updating: content/data/dump/yue/Arg_PISARapaNui.csv (deflated 92%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fa709f87-85d3-4197-9e57-cfb09ce67bcc\", \"data_dump.zip\", 183852)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
